seed: 42

# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
models:
  separate: True
  policy:  # see gaussian_model parameters
    class: GaussianMixin
    deterministic_evaluation: True  # use mean for evaluation instead of sampling
    clip_actions: False
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0
    network:
      - name: net
        input: "STATES"
        layers: [128, 64]
        activations: elu
    output: "ACTIONS"
  value:  # this will be the Q1 and Q2 networks
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: states_net
        input: "STATES" 
        layers: [128, 64]
        activations: elu
      - name: actions_net
        input: "ACTIONS"
        layers: [64]
        activations: elu
      - name: combined_net
        input: ["states_net", "actions_net"]
        layers: [64]
        activations: elu
    output: "ONE"


# Replay memory, as SAC is an off-policy algorithm
# https://skrl.readthedocs.io/en/latest/api/memories/random.html
memory:
  class: RandomMemory
  memory_size: 1000000  # much larger memory than for on-policy algorithms
  num_samples: 256  # batch size


# SAC agent configuration (field names are from SAC_DEFAULT_CONFIG)
# https://skrl.readthedocs.io/en/latest/api/agents/sac.html
agent:
  class: SAC
  polyak: 0.005  # for soft update of target networks
  learning_rate: 3.0e-04
  learning_rate_scheduler: null
  learning_rate_scheduler_kwargs: null
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs: null
  reward_preprocessor: null
  reward_preprocessor_kwargs: null
  random_timesteps: 5000  # initial random exploration
  learning_starts: 5000
  grad_norm_clip: 1.0
  # SAC specific parameters
  auto_alpha: True  # automatically adjust temperature
  alpha: 0.2  # initial temperature (only relevant if auto_alpha is False)
  alpha_learning_rate: 3.0e-04  # learning rate for temperature adjustment
  target_entropy: auto  # if auto, use -dim(A) as target entropy
  # logging and checkpoint
  experiment:
    directory: "ur5_reach_sac"
    experiment_name: ""
    write_interval: auto
    checkpoint_interval: auto


# Sequential trainer
# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
trainer:
  class: SequentialTrainer
  timesteps: 45000
  environment_info: log