# Seed for reproducibility
seed: 42

# Models configuration for multi-observation space
models:
  # Use a shared model for both policy and value (separate: False)
  separate: False
  
  policy:
    class: GaussianMixin # For continuous actions
    clip_actions: False
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0
    
    # Network architecture for multi-input (state + image)
    network:
      # CNN encoder for image input
      - name: camera_features
        input: permute(OBSERVATIONS["image"], (0, 3, 1, 2))    # This will receive the 'image' key from observation dict
        layers:
          - conv2d: {out_channels: 32, kernel_size: 8, stride: 4, padding: 0}
          - conv2d: {out_channels: 64, kernel_size: 4, stride: 2, padding: 0}
          - conv2d: {out_channels: 64, kernel_size: 3, stride: 1, padding: 0}
          - flatten
          - linear: 512
        activations: [relu, relu, relu, null, tanh]

      # MLP encoder for state input
      - name: proprioception
        input: OBSERVATIONS["state"]  # This will receive the 'state' key from observation dict
        layers: [256, 128, 64]
        activations: [relu, relu, relu]

      # Fusion layer combining CNN and MLP outputs
      - name: net
        input: concatenate([camera_features, proprioception])  # Combine features
        layers: [128, 64]
        activations: [relu, relu]

    # Output specification
    output: ACTIONS
    
  value:
    class: DeterministicMixin  # For value estimation
    clip_actions: False
    
    # Same network architecture as policy (since separate: False)
    network:
      - name: camera_features
        input: permute(OBSERVATIONS["image"], (0, 3, 1, 2))  # PyTorch NHWC -> NCHW. Warning: don't permute for JAX since it expects NHWC
        layers:
          - conv2d: {out_channels: 32, kernel_size: 8, stride: 4, padding: 0}
          - conv2d: {out_channels: 64, kernel_size: 4, stride: 2, padding: 0}
          - conv2d: {out_channels: 64, kernel_size: 3, stride: 1, padding: 0}
          - flatten
          - linear: 512
        activations: [relu, relu, relu, null, tanh]
        
      # MLP encoder for state input  
      - name: proprioception
        input: OBSERVATIONS["state"]  # This will receive the 'state' key from observation dict
        layers: [256, 128, 64]
        activations: [ relu, relu, relu]

      # Fusion layer
      - name: net
        input: concatenate([camera_features, proprioception]) 
        layers: [128, 64]
        activations: [relu, relu]

    # Single value output
    output: ONE

# Memory configuration
memory:
  class: RandomMemory
  memory_size: -1  # Automatically determined based on rollouts

# PPO agent configuration
agent:
  class: PPO
  rollouts: 24  # Number of steps before update
  learning_epochs: 10  # Number of epochs per update
  mini_batches: 32  # Number of mini-batches per epoch
  
  # Core PPO parameters
  discount_factor: 0.99
  lambda: 0.95
  learning_rate: 5.0e-04  # Slightly lower for stability with images
  learning_rate_scheduler: KLAdaptiveLR
  learning_rate_scheduler_kwargs:
    kl_threshold: 0.01

  # State preprocessing (for the 'state' part of observations)
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs: null
    
  # Value preprocessing
  value_preprocessor: RunningStandardScaler
  value_preprocessor_kwargs: null
  
  # PPO-specific parameters
  random_timesteps: 0
  learning_starts: 0
  grad_norm_clip: 1.0
  ratio_clip: 0.2
  value_clip: 0.2
  entropy_loss_scale: 0.4 # Higher for exploration with visual input
  clip_predicted_values: True
  value_loss_scale: 1.0
  kl_threshold: 0.01
  rewards_shaper: 1.0
  time_limit_bootstrap: False
  # mixed_precision: True
  # Experiment tracking
  experiment:
    directory: "./logs/skrl_camera_pose_tracking"
    experiment_name: "ppo_multiobs"
    write_interval: 100
    checkpoint_interval: 10000
    wandb: True  # Set to True if using Weights & Biases

# Sequential trainer configuration
trainer:
  class: SequentialTrainer
  timesteps: 150000  # Total training timesteps
  environment_info: log